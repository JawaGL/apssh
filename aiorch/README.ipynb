{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tool is still in search of a name; I haven't decided yet whether it should become a lib  of its own, or be made a part of `apssh` (like it is for now) or of something else like `rhubarbe`. My gut feeling is for the former. \n",
    "\n",
    "Candidate names\n",
    "\n",
    "* `aorch`\n",
    "* `asyncorch`\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case, the purpose is to come up with an as-simple-as-it-gets replacement for our toolset for orchestrating network experiments. In its simplest form, it can be described as an ***orchestration tool for `asyncio`-based libraries***, with the following objectives\n",
    "\n",
    "* we ***primarily*** target ***ssh-based*** kind of interactions with nodes; typically we need to control any number of nodes reachable through ssh, ranging from hundreds of them in the context of PlanetLab, down to tens or a handful in the context of R2lab\n",
    "* previous tools like in particular [NEPI](http://nepi.inria.fr) came with a very ambitious goal of being extremely generic, to the cost of achieving poor to very poor performance, in particular in the specific niche of ssh-addressable nodes; in contrast, here we want to achieve optimal performance, to the possible cost of generality.\n",
    "\n",
    "So in order to take these objectives into account:\n",
    "\n",
    "* we want to have full control on ssh connections, and specifically to open only one such connection per node for the whole duration of the exp.\n",
    "* `asyncio` allows us to be totally single-threaded, so no multi-threading is needed, and thus no critical section nonsense\n",
    "* similarly we want to be able to rely on the internal ssh protocol to be notified when a remote command is done, ***instead of having to cyclically*** check for its status, which comes at an incredibly high cost\n",
    "\n",
    "It appears that the only critical feature required here as compared to what `asyncio` offers out of the box, is to handle dependencies between atomic jobs. It the main and only purpose of this micro-tool, to allow an experimenter to describe its experiment as a logically ordered set of jobs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I hope it will turn out, all this applies in a straightforward way to both\n",
    "\n",
    "* simple ssh-derivatives; at this point, it looks like we essentially need 2 first-class citizens here:\n",
    "  * running commands native to the remote system, or \n",
    "  * pushing a local script remotely and run it\n",
    "\n",
    "* but in fact the same applies as-is to any kind of coroutine, that could either\n",
    "  * have a local purpose, like dealing with a local software bus for exchanging messages between jobs\n",
    "  * or at the other extreme of the spectrum, interact with network resource using technologies totally different from `ssh`, provided that they rely on `asyncio`-aware libraries. Virtually everything is available as `asyncio`-compliant these days, from `http` to `telnet` - already used e.g. in `rhubarbe` - to, I am sure, `xmlrpc` or other more modern variants based on JSON, as well as XMPP-based stuff, if need be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple coroutine for the sake of illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "async def mycoro(timeout):\n",
    "    print(\"-> {} mycoro({})\".format(time.strftime(\"%H:%M:%S\"), timeout))\n",
    "    await asyncio.sleep(timeout)\n",
    "    print(\"<- {} mycoro({})\".format(time.strftime(\"%H:%M:%S\"), timeout))\n",
    "    # return something easy to recognize\n",
    "    return 1000 * timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a series of coroutines in parallel - a la `gather` - can be done like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from job import Job\n",
    "from engine import Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a1, a2, a3 = Job(mycoro(1)), Job(mycoro(2)), Job(mycoro(1.5)),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're saying here is that we have three jobs, that have no relationships between them. \n",
    "\n",
    "So when we run them, we would start all 3 coroutines at once, and return once they are all done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 22:46:19 mycoro(2)\n",
      "-> 22:46:19 mycoro(1.5)\n",
      "-> 22:46:19 mycoro(1)\n",
      "<- 22:46:20 mycoro(1)\n",
      "<- 22:46:20 mycoro(1.5)\n",
      "<- 22:46:21 mycoro(2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ea = Engine(a1, a2, a3)\n",
    "ea.orchestrate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example B : add requirements (dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add *requirements* dependencies between jobs, like in the following example. We take this chance to show that jobs can be tagged with a label, which can turn out te be convenient somtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b1, b2, b3 = (Job(mycoro(1), label=\"b1\"),\n",
    "              Job(mycoro(2)), \n",
    "              Job(mycoro(1.5), label=\"b3\"))\n",
    "\n",
    "b3.requires(b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `b3` needs `b1` to be finished before it can start. And so only the 2 first coroutines get started at the beginning, and only once b1 has finished does b3 start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eb = Engine(b1, b2, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 22:46:21 mycoro(1)\n",
      "-> 22:46:21 mycoro(2)\n",
      "<- 22:46:22 mycoro(1)\n",
      "-> 22:46:22 mycoro(1.5)\n",
      "<- 22:46:23 mycoro(2)\n",
      "<- 22:46:23 mycoro(1.5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eb.orchestrate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inspect results\n",
    "\n",
    "Before we see more examples, let's see how details for each `Job` can be retrieved once `orchestrate` finishes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Job `b1' finished -> 1000 - [allows [b3]]>\n",
      "<Job `NOLABEL' finished -> 2000>\n",
      "<Job `b3' finished -> 1500.0 - [requires [b1]]>\n"
     ]
    }
   ],
   "source": [
    "# a shorter equivalent form would be \n",
    "# e2.list()\n",
    " \n",
    "for job in eb.jobs:\n",
    "    print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(b1.is_done())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500.0\n"
     ]
    }
   ],
   "source": [
    "print(b3.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example C : infinite loops, or coroutines that don't return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is useful to deal with a endless loop; for example if we want to separate completely actions and printing, we can use an `asyncio.Queue` to implement a simple message bus as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "message_bus = asyncio.Queue()\n",
    "\n",
    "async def monitor_loop(bus):\n",
    "    while True:\n",
    "        message = await bus.get()\n",
    "        print(\"{} BUS: {}\".format(time.strftime(\"%H:%M:%S\"), message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a modified version of the previous coroutine, that interacts with this message bus instead of printing anything itself&nbsp;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def mycoro_bus(timeout, bus):\n",
    "    await bus.put(\"-> mycoro({})\".format(timeout))\n",
    "    await asyncio.sleep(timeout)\n",
    "    await bus.put(\"<- mycoro({})\".format(timeout))\n",
    "    # return something easy to recognize\n",
    "    return 10 * timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replay the prevous scenario, adding the monitoring loop as a separate job; however we need to declare this job with `forever=True` so that we know when the bulk of the scenario is completed, since the monitoring loop will never return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:46:23 BUS: -> mycoro(0.8)\n",
      "22:46:23 BUS: -> mycoro(0.4)\n",
      "22:46:24 BUS: <- mycoro(0.4)\n",
      "22:46:24 BUS: -> mycoro(0.6)\n",
      "22:46:24 BUS: <- mycoro(0.8)\n",
      "22:46:24 BUS: <- mycoro(0.6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1, c2, c3, c4 = (Job(mycoro_bus(0.4, message_bus), label=\"c1\"),\n",
    "                  Job(mycoro_bus(0.8, message_bus), label=\"c2\"), \n",
    "                  Job(mycoro_bus(0.6, message_bus), label=\"c3\"),\n",
    "                  Job(monitor_loop(message_bus), forever=True, label=\"monitor\"))\n",
    "\n",
    "c3.requires(c1)\n",
    "\n",
    "ec = Engine(c1, c2, c3, c4)\n",
    "ec.orchestrate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `orchestrate` always terminates as soon as all the non-`forever` jobs are complete. The `forever` jobs, on the other hand, get cancelled, so of course no return value is available at the end of the scenario&nbsp;:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <Job `monitor'[∞] cancelled>\n",
      "1 <Job `c2' finished -> 8.0>\n",
      "2 <Job `c3' finished -> 6.0 - [requires [c1]]>\n",
      "3 <Job `c1' finished -> 4.0 - [allows [c3]]>\n"
     ]
    }
   ],
   "source": [
    "ec.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example D : specifying a global timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`orchestrate` accepts a `timeout` argument in seconds. When provided, `orchestrate` will ensure its global duration does not exceed this value, and will return `False` if the timeout triggers.\n",
    "\n",
    "Of course this can be used with any number of jobs and dependencies, but for the sake of simplicity let us see this in action with just one job that loops forever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:46:25: forever 0\n",
      "22:46:26: forever 1\n",
      "22:46:27: forever 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async def forever():\n",
    "    for i in range(100000):\n",
    "        print(\"{}: forever {}\".format(time.strftime(\"%H:%M:%S\"), i))\n",
    "        await asyncio.sleep(1)\n",
    "        \n",
    "j = Job(forever(), forever=True)\n",
    "e = Engine(j)\n",
    "e.orchestrate(timeout=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the result of `orchestrate` in this case is `False`, since not all jobs have completed. Apart from that the jobs is now in this state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Job `NOLABEL'[∞] cancelled>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### customizing the `Job` class\n",
    "\n",
    "`Job` actually is a specializtion of `AbstractJob`, and the specification is that the `corun()` method should denote a coroutine itself, and that is what is triggered by `Engine` for running said job.\n",
    "\n",
    "You can define your own `Job` class by specializing `job.AbstractJob` - more on this later, we'll define some predefined jobs, in particular for interacting through ssh, and possibly many others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "This totally is only a seed at this point, like day d+1 \n",
    "\n",
    "## deal with exceptions\n",
    "* for now we kind of assume that `corun()` does not trigger an exception. This needs to be robustified. \n",
    "\n",
    "## termination\n",
    "\n",
    "1. provide a means to tidy up jobs once the engine has run out. Typically we would have several jobs using the same ssh connection, and these need to be closed at some point. Something like `Engine.shutdown` sending `Job.coshutdown()`, or similar...\n",
    "\n",
    "  Would it work to just send e.g. `coshutdown()` on all jobs ? this needs a little more thinking though; that could mean trying to shutdown an ssh connection from several points at the same time ...\n",
    "\n",
    "\n",
    "## monitoring \n",
    "* come up with some basic (curses ?) monitor to show what's going on; what I have in mind is something like rhubarbe load where all jobs would be displayed, one line each, and their status could be shown so that one can get a sense of what is going on\n",
    "* one way to look at this is to have the main Engine class send itself a `tick()` method, and then specialize `Engine` as `EngineCurses` that would actually do things on such events.\n",
    "* ***or*** this gets delegated on a `message_queue` object. **Review the rhubarbe code on this aspect**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
